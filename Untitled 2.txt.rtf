{\rtf1\ansi\ansicpg1252\cocoartf2636
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red253\green128\blue8;\red255\green255\blue10;\red33\green255\blue6;
\red0\green0\blue0;\red33\green255\blue255;\red251\green2\blue255;\red253\green128\blue8;\red0\green0\blue0;
}
{\*\expandedcolortbl;;\cssrgb\c100000\c57637\c0;\cssrgb\c99942\c98555\c0;\cssrgb\c0\c97680\c0;
\csgray\c0;\cssrgb\c0\c99144\c100000;\cssrgb\c100000\c25279\c100000;\cssrgb\c100000\c57637\c0;\csgray\c0\c0;
}
\paperw11900\paperh16840\margl1440\margr1440\vieww14180\viewh17220\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 (1) ----------------------------------------------------------------------------------------------\
\
We consider embedding piecewise linear graph neural networks (ReLU networks) as surrogate models in mixed integer linear programming (MILP) problems. A MILP formulation of multi layer perceptron (MLP) ReLU networks has recently been applied by many authors to probe for various model properties subject to input bounds. The formulation is obtained by programming each ReLU operator with a binary variable and applying the big-M method. The MILP formulation of Neural Networks was yet to be extended to include Graph Neural Networks. Graph Neural Networks have shown to effectively model properties of non-euclidean data sets. Our contribution includes a novel single-layer linear formulation and a multilayer bi-linear MILP formulation of the graph convolutional network (GCN) by Kipf and Willing (2016). Our research also includes a linear multilayered MILP formulation of the graphconv network, which can be formulated using fewer constraints and variables. Numerical results show that we can achieve similar model accuracy while reducing solve times to optimality using graphconv over GCNs. Finally bound-tightening techniques for MILP formulations are presented further reducing solving times. \
\
(2) ----------------------------------------------------------------------------------------------\
\
We consider embedding piecewise linear graph neural networks (ReLU networks) as surrogate models in mixed integer linear programming (MILP) problems. A MILP formulation of multi layer perceptron (MLP) ReLU networks has recently been applied by many authors to probe for various model properties subject to input bounds. The formulation is obtained by programming each ReLU operator with a binary variable and applying the big-M method. The MILP formulation of Neural Networks was yet to be extended to include Graph Neural Networks. Graph Neural Networks have shown to effectively model properties of non-euclidean data sets. Our contribution includes a novel single-layer linear formulation and a multilayer bi-linear MILP formulation of the graph convolutional network (GCN) by Kipf and Willing (2016). Our research also includes a linear multilayered MILP formulation of the graphconv network, which can be formulated using fewer constraints and variables. Preliminary computational results are reported, aimed at investigating the computational performance of the different models, when applied to the non-euclidean data set of boiling points of molecules. Performance is improved by implementing bound tightening techniques. \
\
(3) -----------------------------------------------------------------------------------------------\
\
\cb2 context\cb1 \
\cb3 research gap\
\cb4 what of our contribution\
\cf5 \cb6 how of our contribution\
\cf0 \cb7 results of our contribution\
\cb1 \
\cb2 Embedding piecewise linear neural networks (ReLU networks) as surrogate models in mixed integer linear programming (MILP) problems enables the incorporation of complex non-linear functions while searching for a property \cb8 global \cb2 optima of a dataset. An MILP formulation using the big-M method of multi layer perceptron (MLP) ReLU networks has recently been applied by many authors to probe for various model properties subject to input bounds\cb1 . \cb3 Properties of many data with non-euclidean data structures are modeled more effectively by Graph Convolutional Neural Networks (GCN) than MLPs. No MILP formulations of GCNs exist to our knowledge.\cb1  \cb4 This contribution extends MI(N)LP formulations of neural networks to include Graph Neural Networks. Our contribution includes a bilinear MINLP formulation of the GCN, and a linear formulation of the graphconv neural network. \cb6 We compare our formulations to an interior point (local) optimizer by comparing solving times and optimality gaps while modelling a dataset of boiling points of different molecules.\cb9  [our hypothesis]\cb7  Our research finds that, although slower, our formulation finds global optima more consistently than the interior point method. Between our two formulations the graphconv neural network achieves similar model accuracy, and achieves faster solving times when embedded as a surrogate model in an MILP problem. As a final contribution we find that bound tightening optimized for graph neural networks further speed up solving times. \cb9 \
\
Compari\
\
\
1. What I am scared of is that graphconv and GCNs don\'92t find the same solution. \
2. What if the local optimizers find the same answers? Does this imply it\'92s not publishable or is it also interesting ?\
3. Since the machine learning model is an approximation, how can you confirm an optimum? The global optimizer finds a solution restricted by the model but }